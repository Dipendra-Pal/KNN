{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDNAkh4qDkcYi2U2QHxpQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipendra-Pal/KNN/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PzR16TpqrNe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron class\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_inputs=2):\n",
        "        self.weights = np.random.rand(n_inputs)\n",
        "        self.bias = np.random.rand(1)\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        return self.activation(weighted_sum)\n",
        "\n",
        "    def train(self, X, y, max_epochs=100):\n",
        "        print(f\"\\nTraining Perceptron...\")\n",
        "        print(f\"Initial weights: {self.weights}, Initial bias: {self.bias[0]:.4f}\")\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            error_count = 0\n",
        "            for inputs, target in zip(X, y):\n",
        "                prediction = self.predict(inputs)\n",
        "                error = target - prediction\n",
        "\n",
        "                if error != 0:\n",
        "                    error_count += 1\n",
        "                    self.weights += self.lr * error * inputs\n",
        "                    self.bias += self.lr * error\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}: Weights: {self.weights}, Bias: {self.bias[0]:.4f}, Errors: {error_count}\")\n",
        "\n",
        "            if error_count == 0:\n",
        "                print(f\"Converged after {epoch + 1} epochs\")\n",
        "                break\n",
        "\n",
        "        return self.weights, self.bias\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "        correct = 0\n",
        "        for inputs, target in zip(X, y):\n",
        "            prediction = self.predict(inputs)\n",
        "            if prediction == target:\n",
        "                correct += 1\n",
        "        return correct / len(y)\n"
      ],
      "metadata": {
        "id": "oL_0FYYRsXcl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for AND and OR gates\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "y_or = np.array([0, 1, 1, 1])\n",
        "\n",
        "# Save datasets to CSV\n",
        "and_data = pd.DataFrame({\n",
        "    'Input1': X[:, 0],\n",
        "    'Input2': X[:, 1],\n",
        "    'AND_Output': y_and\n",
        "})\n",
        "or_data = pd.DataFrame({\n",
        "    'Input1': X[:, 0],\n",
        "    'Input2': X[:, 1],\n",
        "    'OR_Output': y_or\n",
        "})"
      ],
      "metadata": {
        "id": "7C0eH3h9sp16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Perceptron for AND gate\n",
        "print(\"=== AND Gate Training ===\")\n",
        "and_perceptron = Perceptron(learning_rate=0.1)\n",
        "final_weights_and, final_bias_and = and_perceptron.train(X, y_and)\n",
        "accuracy_and = and_perceptron.accuracy(X, y_and)\n",
        "print(f\"\\nFinal Weights for AND: {final_weights_and}\")\n",
        "print(f\"Final Bias for AND: {final_bias_and[0]:.4f}\")\n",
        "print(f\"Accuracy for AND: {accuracy_and * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOxyTMh2s9CW",
        "outputId": "b3937454-694d-4a0f-cdb4-14239f20e1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AND Gate Training ===\n",
            "\n",
            "Training Perceptron...\n",
            "Initial weights: [0.63482377 0.24113129], Initial bias: 0.7898\n",
            "Epoch 1: Weights: [0.53482377 0.14113129], Bias: 0.4898, Errors: 3\n",
            "Epoch 2: Weights: [0.43482377 0.04113129], Bias: 0.1898, Errors: 3\n",
            "Epoch 3: Weights: [ 0.33482377 -0.05886871], Bias: -0.1102, Errors: 3\n",
            "Epoch 4: Weights: [0.33482377 0.04113129], Bias: -0.1102, Errors: 2\n",
            "Epoch 5: Weights: [0.23482377 0.04113129], Bias: -0.2102, Errors: 1\n",
            "Epoch 6: Weights: [0.23482377 0.14113129], Bias: -0.2102, Errors: 2\n",
            "Epoch 7: Weights: [0.23482377 0.24113129], Bias: -0.2102, Errors: 2\n",
            "Epoch 8: Weights: [0.23482377 0.14113129], Bias: -0.3102, Errors: 1\n",
            "Epoch 9: Weights: [0.23482377 0.14113129], Bias: -0.3102, Errors: 0\n",
            "Converged after 9 epochs\n",
            "\n",
            "Final Weights for AND: [0.23482377 0.14113129]\n",
            "Final Bias for AND: -0.3102\n",
            "Accuracy for AND: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Perceptron for OR gate\n",
        "print(\"\\n=== OR Gate Training ===\")\n",
        "or_perceptron = Perceptron(learning_rate=0.1)\n",
        "final_weights_or, final_bias_or = or_perceptron.train(X, y_or)\n",
        "accuracy_or = or_perceptron.accuracy(X, y_or)\n",
        "print(f\"\\nFinal Weights for OR: {final_weights_or}\")\n",
        "print(f\"Final Bias for OR: {final_bias_or[0]:.4f}\")\n",
        "print(f\"Accuracy for OR: {accuracy_or * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDhkX7C7tNZ1",
        "outputId": "ae8d618e-5ea3-427c-e42e-3d08ab024bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== OR Gate Training ===\n",
            "\n",
            "Training Perceptron...\n",
            "Initial weights: [0.49504082 0.99757244], Initial bias: 0.8422\n",
            "Epoch 1: Weights: [0.49504082 0.99757244], Bias: 0.7422, Errors: 1\n",
            "Epoch 2: Weights: [0.49504082 0.99757244], Bias: 0.6422, Errors: 1\n",
            "Epoch 3: Weights: [0.49504082 0.99757244], Bias: 0.5422, Errors: 1\n",
            "Epoch 4: Weights: [0.49504082 0.99757244], Bias: 0.4422, Errors: 1\n",
            "Epoch 5: Weights: [0.49504082 0.99757244], Bias: 0.3422, Errors: 1\n",
            "Epoch 6: Weights: [0.49504082 0.99757244], Bias: 0.2422, Errors: 1\n",
            "Epoch 7: Weights: [0.49504082 0.99757244], Bias: 0.1422, Errors: 1\n",
            "Epoch 8: Weights: [0.49504082 0.99757244], Bias: 0.0422, Errors: 1\n",
            "Epoch 9: Weights: [0.49504082 0.99757244], Bias: -0.0578, Errors: 1\n",
            "Epoch 10: Weights: [0.49504082 0.99757244], Bias: -0.0578, Errors: 0\n",
            "Converged after 10 epochs\n",
            "\n",
            "Final Weights for OR: [0.49504082 0.99757244]\n",
            "Final Bias for OR: -0.0578\n",
            "Accuracy for OR: 100.00%\n"
          ]
        }
      ]
    }
  ]
}